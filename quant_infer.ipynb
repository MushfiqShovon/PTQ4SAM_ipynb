{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7b3734",
   "metadata": {},
   "source": [
    "# Quantized PTQ4SAM Inference (No PTQ)\n",
    "Load a quantized SAM predictor from a .pth file and run inference/evaluation on the COCO val2017 test split without running the PTQ pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fcd989",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configure Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67527941",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM\n",
      "Predictor: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/quant_sam_predictor.pth\n",
      "Work dir: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6\n",
      "Subset ann: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/instances_val2017_10.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "REPO_ROOT = \"/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM\"\n",
    "CONFIG_PATH = os.path.join(REPO_ROOT, \"projects/configs/yolox/yolo_l-sam-vit-b.py\")\n",
    "PREDICTOR_PTH = os.path.join(REPO_ROOT, \"result/yolox_l_vitb_w6a6/quant_sam_predictor.pth\")\n",
    "WORK_DIR = os.path.join(REPO_ROOT, \"result/yolox_l_vitb_w6a6\")\n",
    "CONDA_ENV = os.path.join(REPO_ROOT, \".ptq4sam\")\n",
    "\n",
    "# Subset settings\n",
    "NUM_IMAGES = 10\n",
    "SUBSET_ANN = os.path.join(WORK_DIR, f\"instances_val2017_{NUM_IMAGES}.json\")\n",
    "\n",
    "print(\"Repo:\", REPO_ROOT)\n",
    "print(\"Predictor:\", PREDICTOR_PTH)\n",
    "print(\"Work dir:\", WORK_DIR)\n",
    "print(\"Subset ann:\", SUBSET_ANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ff661",
   "metadata": {},
   "source": [
    "## 2. Load Quantized Model from `.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b99026",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval command:\n",
      "conda run -p /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam python ptq4sam/solver/test_quant.py --config /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/configs/yolox/yolo_l-sam-vit-b.py --load_sam_path /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/quant_sam_predictor.pth --work-dir /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6 --eval segm --cfg-options data.test.ann_file=/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/instances_val2017_10.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import http.cookiejar\n",
    "\n",
    "# Verify the quantized predictor exists (no PTQ here)\n",
    "GDRIVE_FILE_ID = \"103Y5SadTarO4obWzNnWb0jVDnVzrGn3j\"\n",
    "\n",
    "\n",
    "def _download_gdrive(file_id, dest_path, chunk_size=1024 * 1024):\n",
    "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    cookie_jar = http.cookiejar.CookieJar()\n",
    "    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n",
    "\n",
    "    response = opener.open(url)\n",
    "    token = None\n",
    "    for c in cookie_jar:\n",
    "        if c.name.startswith(\"download_warning\"):\n",
    "            token = c.value\n",
    "            break\n",
    "    if token:\n",
    "        response = opener.open(url + f\"&confirm={token}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    with open(dest_path, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "if not os.path.isfile(PREDICTOR_PTH):\n",
    "    print(\"Quantized predictor not found. Downloading from Google Drive...\")\n",
    "    _download_gdrive(GDRIVE_FILE_ID, PREDICTOR_PTH)\n",
    "\n",
    "if not os.path.isfile(PREDICTOR_PTH):\n",
    "    raise FileNotFoundError(f\"Missing predictor pth: {PREDICTOR_PTH}\")\n",
    "\n",
    "# Build the command to evaluate using the ptq4sam environment\n",
    "cmd = [\n",
    "    \"conda\", \"run\", \"-p\", CONDA_ENV,\n",
    "    \"python\", \"ptq4sam/solver/test_quant.py\",\n",
    "    \"--config\", CONFIG_PATH,\n",
    "    \"--load_sam_path\", PREDICTOR_PTH,\n",
    "    \"--work-dir\", WORK_DIR,\n",
    "    \"--eval\", \"segm\",\n",
    "    \"--cfg-options\", f\"data.test.ann_file={SUBSET_ANN}\",\n",
    "]\n",
    "\n",
    "print(\"Eval command:\")\n",
    "print(\" \".join(cmd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94382c76",
   "metadata": {},
   "source": [
    "## 3. Define Dataset and DataLoader for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cea37a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO val2017 looks present.\n",
      "Subset annotations ready: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/instances_val2017_10.json\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset sanity checks (COCO val2017) + make a 10-image subset\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "ann_path = os.path.join(REPO_ROOT, \"data/coco/annotations/instances_val2017.json\")\n",
    "img_dir = os.path.join(REPO_ROOT, \"data/coco/val2017\")\n",
    "\n",
    "data_root = os.path.join(REPO_ROOT, \"data/coco\")\n",
    "ann_zip = os.path.join(data_root, \"annotations_trainval2017.zip\")\n",
    "val_zip = os.path.join(data_root, \"val2017.zip\")\n",
    "\n",
    "# Auto-download COCO val2017 + annotations if missing\n",
    "if not os.path.isfile(ann_path):\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    if not os.path.isfile(ann_zip):\n",
    "        print(\"Downloading COCO annotations...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "            ann_zip,\n",
    "        )\n",
    "    print(\"Extracting annotations...\")\n",
    "    with zipfile.ZipFile(ann_zip, \"r\") as zf:\n",
    "        zf.extractall(data_root)\n",
    "\n",
    "if not os.path.isdir(img_dir):\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    if not os.path.isfile(val_zip):\n",
    "        print(\"Downloading COCO val2017 images...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "            val_zip,\n",
    "        )\n",
    "    print(\"Extracting val2017 images...\")\n",
    "    with zipfile.ZipFile(val_zip, \"r\") as zf:\n",
    "        zf.extractall(data_root)\n",
    "\n",
    "if not os.path.isfile(ann_path):\n",
    "    raise FileNotFoundError(f\"Missing COCO annotations: {ann_path}\")\n",
    "if not os.path.isdir(img_dir):\n",
    "    raise FileNotFoundError(f\"Missing COCO images dir: {img_dir}\")\n",
    "\n",
    "if not os.path.isfile(SUBSET_ANN):\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "    images = coco.get(\"images\", [])[:NUM_IMAGES]\n",
    "    image_ids = {img[\"id\"] for img in images}\n",
    "    annotations = [ann for ann in coco.get(\"annotations\", []) if ann.get(\"image_id\") in image_ids]\n",
    "    subset = {\n",
    "        \"info\": coco.get(\"info\", {}),\n",
    "        \"licenses\": coco.get(\"licenses\", []),\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": coco.get(\"categories\", []),\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(SUBSET_ANN), exist_ok=True)\n",
    "    with open(SUBSET_ANN, \"w\") as f:\n",
    "        json.dump(subset, f)\n",
    "\n",
    "print(\"COCO val2017 looks present.\")\n",
    "print(f\"Subset annotations ready: {SUBSET_ANN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390c37",
   "metadata": {},
   "source": [
    "## 4. Run Inference and Collect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c784b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return code: 0\n",
      "--- STDOUT ---\n",
      "projects.instance_segment_anything\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=15.99s)\n",
      "creating index...\n",
      "index created!\n",
      "the length of cali data is 32.\n",
      "load checkpoint from local path: ./ckpt/yolox_l.pth\n",
      "[                                                  ] 0/10, elapsed: 0s, ETA:\n",
      "[>>>                               ] 1/10, 0.1 task/s, elapsed: 7s, ETA:    64s\n",
      "[>>>>>>                            ] 2/10, 0.3 task/s, elapsed: 8s, ETA:    31s\n",
      "[>>>>>>>>>>                        ] 3/10, 0.4 task/s, elapsed: 8s, ETA:    20s\n",
      "[>>>>>>>>>>>>>                     ] 4/10, 0.4 task/s, elapsed: 9s, ETA:    14s\n",
      "[>>>>>>>>>>>>>>>>                 ] 5/10, 0.5 task/s, elapsed: 10s, ETA:    10s\n",
      "[>>>>>>>>>>>>>>>>>>>              ] 6/10, 0.6 task/s, elapsed: 10s, ETA:     7s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>          ] 7/10, 0.6 task/s, elapsed: 11s, ETA:     5s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>       ] 8/10, 0.7 task/s, elapsed: 12s, ETA:     3s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 9/10, 0.7 task/s, elapsed: 12s, ETA:     1s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.8 task/s, elapsed: 13s, ETA:     0s\n",
      "Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.09s).\n",
      "\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.601\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.367\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.252\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.534\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.435\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.299\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.542\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.487\n",
      "\n",
      "OrderedDict([('segm_mAP', 0.368), ('segm_mAP_50', 0.601), ('segm_mAP_75', 0.367), ('segm_mAP_s', 0.252), ('segm_mAP_m', 0.534), ('segm_mAP_l', 0.435), ('segm_mAP_copypaste', '0.368 0.601 0.367 0.252 0.534 0.435')])\n",
      "\n",
      "\n",
      "--- STDERR ---\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/mmdetection/mmdet/utils/compat_config.py:28: UserWarning: config is now expected to have a `runner` section, please set `runner` in your config.\n",
      "  warnings.warn(\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/mmdetection/mmdet/utils/setup_env.py:38: UserWarning: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "  warnings.warn(\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/mmdetection/mmdet/utils/setup_env.py:48: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "  warnings.warn(\n",
      "2026-02-11 19:05:14,495 - ptq4sam - INFO - load sam:\n",
      "2026-02-11 19:05:14,495 - ptq4sam - INFO - /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/quant_sam_predictor.pth\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/mmdetection/mmdet/datasets/coco.py:470: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "2026-02-11 19:06:18,737 - ptq4sam - INFO - {'a_qconfig': {'quantizer': 'LSQFakeQuantize', 'observer': 'AvgMinMaxObserver', 'bit': 6, 'symmetric': False, 'ch_axis': -1}, 'w_qconfig': {'quantizer': 'AdaRoundFakeQuantize', 'observer': 'MSEObserver', 'bit': 6, 'symmetric': False, 'ch_axis': 0}, 'calibrate': 32, 'recon': {'batch_size': 1, 'scale_lr': 4e-05, 'warm_up': 0.2, 'weight': 0.01, 'iters': 20000, 'b_range': [20, 2], 'keep_gpu': True, 'round_mode': 'learned_hard_sigmoid', 'drop_prob': 0.5}, 'ptq4sam': {'BIG': True, 'AGQ': True, 'global_num': 128, 'peak_distance': 32, 'peak_height': 0.01}}\n",
      "2026-02-11 19:06:19,148 - ptq4sam - INFO - OrderedDict([('segm_mAP', 0.368), ('segm_mAP_50', 0.601), ('segm_mAP_75', 0.367), ('segm_mAP_s', 0.252), ('segm_mAP_m', 0.534), ('segm_mAP_l', 0.435), ('segm_mAP_copypaste', '0.368 0.601 0.367 0.252 0.534 0.435')])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference/evaluation using the quantized predictor (no PTQ)\n",
    "run_env = os.environ.copy()\n",
    "run_env[\"PYTHONPATH\"] = REPO_ROOT\n",
    "\n",
    "result = subprocess.run(cmd, cwd=REPO_ROOT, env=run_env, capture_output=True, text=True)\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"--- STDOUT ---\")\n",
    "print(result.stdout)\n",
    "print(\"--- STDERR ---\")\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Inference failed; see logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be9e63",
   "metadata": {},
   "source": [
    "## 5. Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2c5b2b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest eval: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/eval_20260211_190435.json\n",
      "{'config': '/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/configs/yolox/yolo_l-sam-vit-b.py', 'metric': {'segm_mAP': 0.368, 'segm_mAP_50': 0.601, 'segm_mAP_75': 0.367, 'segm_mAP_s': 0.252, 'segm_mAP_m': 0.534, 'segm_mAP_l': 0.435, 'segm_mAP_copypaste': '0.368 0.601 0.367 0.252 0.534 0.435'}}\n"
     ]
    }
   ],
   "source": [
    "# Load the latest evaluation JSON\n",
    "json_files = sorted(glob.glob(os.path.join(WORK_DIR, \"eval_*.json\")))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No eval_*.json found in {WORK_DIR}\")\n",
    "\n",
    "latest_json = json_files[-1]\n",
    "with open(latest_json, \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"Latest eval:\", latest_json)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64814e4f",
   "metadata": {},
   "source": [
    "## 6. Preview Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5c4a64",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To visualize results, re-run with --show-dir result/yolox_l_vitb_w6a6/vis\n"
     ]
    }
   ],
   "source": [
    "# Previewing predictions is handled by test_quant.py with --show-dir.\n",
    "# If you want visualization, re-run with:\n",
    "#   --show-dir result/yolox_l_vitb_w6a6/vis\n",
    "print(\"To visualize results, re-run with --show-dir result/yolox_l_vitb_w6a6/vis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9dc0a",
   "metadata": {},
   "source": [
    "## 7. Export ONNX and Run with ONNX Runtime\n",
    "This exports the quantized SAM image encoder from the saved predictor. (Exporting the full SAM pipeline requires a custom wrapper with fixed prompt inputs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0514973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export command: conda run -p /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam python /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/export_onnx_image_encoder.py\n",
      "Return code: 0\n",
      "--- STDOUT ---\n",
      "Saved ONNX: /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/quant_sam_image_encoder.onnx\n",
      "\n",
      "\n",
      "--- STDERR ---\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:262: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/ptq4sam/quantization/fake_quant.py:173: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.scale.data.clamp_(min=self.eps.item())\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/ptq4sam/quantization/fake_quant.py:192: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  X, self.scale, self.zero_point.item(), self.quant_min, self.quant_max, grad_factor)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if rel_pos.shape[0] != max_rel_dist:\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:320: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/ptq4sam/quantization/fake_quant.py:591: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  X[softmax_mask] = torch.Tensor([0.0])\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:283: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:284: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/projects/instance_segment_anything/models/segment_anything/modeling/image_encoder.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n",
      "/home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:325: UserWarning: Type cannot be inferred, which might cause exported graph to produce incorrect results.\n",
      "  warnings.warn(\"Type cannot be inferred, which might cause exported graph to produce incorrect results.\")\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7c534ac39d62 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3a9 (0x7c539003e159 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xe6 (0x7c5390040116 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x1d39c02 (0x7c5390739c02 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb9 (0x7c53902d3169 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x32543e3 (0x7c5391c543e3 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x3254a15 (0x7c5391c54a15 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x166 (0x7c5390352f56 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b5f (0x7c5412b756af in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xbbcbc2 (0x7c5412bbcbc2 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0xa8e (0x7c5412bc231e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbc3e24 (0x7c5412bc3e24 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3462c (0x7c5412b3462c in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x2a965e (0x7c54122a965e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #41: <unknown function> + 0x29d90 (0x7c541551dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #42: __libc_start_main + 0x80 (0x7c541551de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      " (function ComputeConstantFolding)\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7c534ac39d62 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3a9 (0x7c539003e159 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xe6 (0x7c5390040116 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x1d39c02 (0x7c5390739c02 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb9 (0x7c53902d3169 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x32543e3 (0x7c5391c543e3 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x3254a15 (0x7c5391c54a15 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x166 (0x7c5390352f56 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b5f (0x7c5412b756af in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xbbcbc2 (0x7c5412bbcbc2 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0xa8e (0x7c5412bc231e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbc3e24 (0x7c5412bc3e24 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3462c (0x7c5412b3462c in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x2a965e (0x7c54122a965e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #41: <unknown function> + 0x29d90 (0x7c541551dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #42: __libc_start_main + 0x80 (0x7c541551de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      " (function ComputeConstantFolding)\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7c534ac39d62 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3a9 (0x7c539003e159 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xe6 (0x7c5390040116 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x1d39c02 (0x7c5390739c02 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb9 (0x7c53902d3169 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x32543e3 (0x7c5391c543e3 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x3254a15 (0x7c5391c54a15 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x166 (0x7c5390352f56 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b5f (0x7c5412b756af in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xbbcbc2 (0x7c5412bbcbc2 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0xa8e (0x7c5412bc231e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbc3e24 (0x7c5412bc3e24 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3462c (0x7c5412b3462c in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x2a965e (0x7c54122a965e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #38: <unknown function> + 0x29d90 (0x7c541551dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #39: __libc_start_main + 0x80 (0x7c541551de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      " (function ComputeConstantFolding)\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7c534ac39d62 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3a9 (0x7c539003e159 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xe6 (0x7c5390040116 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x1d39c02 (0x7c5390739c02 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb9 (0x7c53902d3169 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x32543e3 (0x7c5391c543e3 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x3254a15 (0x7c5391c54a15 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x166 (0x7c5390352f56 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b5f (0x7c5412b756af in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xbbcbc2 (0x7c5412bbcbc2 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0xa8e (0x7c5412bc231e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbc3e24 (0x7c5412bc3e24 in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3462c (0x7c5412b3462c in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x2a965e (0x7c54122a965e in /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #38: <unknown function> + 0x29d90 (0x7c541551dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #39: __libc_start_main + 0x80 (0x7c541551de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      " (function ComputeConstantFolding)\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "\n",
    "onnx_path = os.path.join(WORK_DIR, \"quant_sam_image_encoder.onnx\")\n",
    "export_script = os.path.join(WORK_DIR, \"export_onnx_image_encoder.py\")\n",
    "\n",
    "export_code = f\"\"\"\n",
    "import torch\n",
    "import os\n",
    "from ptq4sam.quantization.fake_quant import QuantizeBase\n",
    "\n",
    "predictor_pth = r\"{PREDICTOR_PTH}\"\n",
    "onnx_path = r\"{onnx_path}\"\n",
    "\n",
    "# Reduce CPU memory pressure and enforce float32\n",
    "torch.set_num_threads(1)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "predictor = torch.load(predictor_pth, map_location=\"cpu\")\n",
    "image_encoder = predictor.model.image_encoder\n",
    "image_encoder = image_encoder.float()\n",
    "image_encoder.eval()\n",
    "\n",
    "# Ensure required quant buffers exist for ONNX export\n",
    "for m in image_encoder.modules():\n",
    "    if isinstance(m, QuantizeBase):\n",
    "        if not hasattr(m, \"scale\"):\n",
    "            m.scale = torch.nn.Parameter(torch.tensor([1.0], dtype=torch.float32))\n",
    "        else:\n",
    "            m.scale.data = m.scale.data.float()\n",
    "        if not hasattr(m, \"zero_point\"):\n",
    "            m.zero_point = torch.tensor([0], dtype=torch.int64)\n",
    "        else:\n",
    "            m.zero_point = m.zero_point.to(torch.int64)\n",
    "\n",
    "# SAM image encoder expects 1024x1024 (positional embedding size)\n",
    "dummy = torch.randn(1, 3, 1024, 1024, dtype=torch.float32)\n",
    "\n",
    "# Export image encoder only\n",
    "# Use external data to reduce peak RAM usage\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        image_encoder,\n",
    "        dummy,\n",
    "        onnx_path,\n",
    "        input_names=[\"image\"],\n",
    "        output_names=[\"embeddings\"],\n",
    "        opset_version=12,\n",
    "        do_constant_folding=False,\n",
    "        export_params=True,\n",
    "        keep_initializers_as_inputs=False,\n",
    "        use_external_data_format=True,\n",
    "        dynamic_axes={{\n",
    "            \"image\": {{2: \"height\", 3: \"width\"}},\n",
    "            \"embeddings\": {{2: \"grid_h\", 3: \"grid_w\"}}\n",
    "        }}\n",
    "    )\n",
    "    print(\"Saved ONNX:\", onnx_path)\n",
    "except TypeError:\n",
    "    # Fallback for older PyTorch without external data flag\n",
    "    torch.onnx.export(\n",
    "        image_encoder,\n",
    "        dummy,\n",
    "        onnx_path,\n",
    "        input_names=[\"image\"],\n",
    "        output_names=[\"embeddings\"],\n",
    "        opset_version=12,\n",
    "        do_constant_folding=False,\n",
    "        export_params=True,\n",
    "        keep_initializers_as_inputs=False,\n",
    "        dynamic_axes={{\n",
    "            \"image\": {{2: \"height\", 3: \"width\"}},\n",
    "            \"embeddings\": {{2: \"grid_h\", 3: \"grid_w\"}}\n",
    "        }}\n",
    "    )\n",
    "    print(\"Saved ONNX:\", onnx_path)\n",
    "\"\"\"\n",
    "\n",
    "with open(export_script, \"w\") as f:\n",
    "    f.write(textwrap.dedent(export_code))\n",
    "\n",
    "# Run export inside ptq4sam env\n",
    "export_cmd = [\n",
    "    \"conda\", \"run\", \"-p\", CONDA_ENV,\n",
    "    \"python\", export_script,\n",
    "]\n",
    "\n",
    "run_env = os.environ.copy()\n",
    "run_env[\"PYTHONPATH\"] = REPO_ROOT\n",
    "\n",
    "print(\"Export command:\", \" \".join(export_cmd))\n",
    "result = subprocess.run(export_cmd, cwd=REPO_ROOT, env=run_env, capture_output=True, text=True)\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"--- STDOUT ---\")\n",
    "print(result.stdout)\n",
    "print(\"--- STDERR ---\")\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"ONNX export failed; see logs above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e9c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNXRuntime command: conda run -p /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/.ptq4sam python /home/jovyan/video-proj-storage/mushfiq_files/PTQ4SAM/PTQ4SAM/result/yolox_l_vitb_w6a6/run_onnx_image_encoder.py\n",
      "Return code: 0\n",
      "--- STDOUT ---\n",
      "ONNX output shapes: [(1, 256, 64, 64)]\n",
      "\n",
      "\n",
      "--- STDERR ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "\n",
    "onnx_path = os.path.join(WORK_DIR, \"quant_sam_image_encoder.onnx\")\n",
    "run_script = os.path.join(WORK_DIR, \"run_onnx_image_encoder.py\")\n",
    "\n",
    "run_code = f\"\"\"\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "onnx_path = r\"{onnx_path}\"\n",
    "if not os.path.isfile(onnx_path):\n",
    "    raise FileNotFoundError(f\"Missing ONNX file: {onnx_path}\")\n",
    "\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.intra_op_num_threads = 1\n",
    "sess_opts.inter_op_num_threads = 1\n",
    "\n",
    "sess = ort.InferenceSession(onnx_path, sess_options=sess_opts, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "image = np.random.randn(1, 3, 1024, 1024).astype(np.float32)\n",
    "outputs = sess.run(None, {{input_name: image}})\n",
    "print(\"ONNX output shapes:\", [o.shape for o in outputs])\n",
    "\"\"\"\n",
    "\n",
    "with open(run_script, \"w\") as f:\n",
    "    f.write(textwrap.dedent(run_code))\n",
    "\n",
    "run_cmd = [\n",
    "    \"conda\", \"run\", \"-p\", CONDA_ENV,\n",
    "    \"python\", run_script,\n",
    "]\n",
    "\n",
    "print(\"ONNXRuntime command:\", \" \".join(run_cmd))\n",
    "result = subprocess.run(run_cmd, cwd=REPO_ROOT, capture_output=True, text=True)\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"--- STDOUT ---\")\n",
    "print(result.stdout)\n",
    "print(\"--- STDERR ---\")\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"ONNXRuntime inference failed; see logs above.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
